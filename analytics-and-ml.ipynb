{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a26c24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################\n",
    "##################\n",
    "\n",
    "\n",
    "def read_data(spark, customSchema):\n",
    "    '''\n",
    "    spark_session: spark\n",
    "    customSchema : we have given the custom schema\n",
    "    '''\n",
    "    print(\"--------------------\")\n",
    "    print (\"Starting read_data\")\n",
    "    print(\"--------------------\")\n",
    "    \n",
    "    #Mention the Bucket name inside the bucket name variable\n",
    "    bucket_name = \"loan-data926529466287266\"\n",
    "    s3_input_path = \"s3://\" + bucket_name + \"/inputfile/loan_data.csv\"\n",
    "    # Read the CSV file into a dataframe. Make sure to give header as true and schema as customSchema.\n",
    "    df = spark.read.csv(s3_input_path, header=True, schema=customSchema)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def clean_data(input_df):\n",
    "    '''\n",
    "    for input file: input_df is output of read_data function\n",
    "    '''\n",
    "    print(\"--------------------\")\n",
    "    print(\"Starting clean_data\")\n",
    "    print(\"--------------------\")\n",
    "    \n",
    "    df = input_df.dropna()    # Drop any rows containing null values.\n",
    "    df = df.dropDuplicates()    # Remove duplicate rows.\n",
    "    df = df.filter(df.purpose!='null')    # Drop the rows where the \"purpose\" column contains the string \"null\".\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def s3_load_data(data, file_name):\n",
    "    '''\n",
    "    data : the output data of refult_1 and result_2 function \n",
    "    file name : the name of the output to be stored inside the s3\n",
    "    '''\n",
    "    #Mention the bucket name inside the bucket_name variable\n",
    "    bucket_name = \"loan-data926529466287266\"\t\n",
    "    output_path = \"s3://\" + bucket_name + \"/output\"+ file_name\n",
    "    \n",
    "    if data.count() !=0:\n",
    "        print(\"Loading the data\", output_path)\n",
    "        # Write a code to store the outputs to the respective locations using the output_path param.\n",
    "        data.coalesce(1).write.csv(output_path, header=True, mode=\"overwrite\")\n",
    "    \n",
    "    else:\n",
    "        print(\"Empty dataframe, hence cannot save the data\", output_path)\n",
    "\n",
    "\n",
    "def result_1(input_df):\n",
    "    '''\n",
    "    for input file: input_df is output of clean_data function\n",
    "    '''\n",
    "    print(\"--------------------------\")\n",
    "    print(\"Starting result_1\")\n",
    "    print(\"--------------------------\")\n",
    "    \n",
    "    # Filters the rows where the \"purpose\" is either \"educational\" or \"small_business\"\n",
    "    df = input_df.filter((col(\"purpose\")==\"educational\")|(col(\"purpose\")==\"small_business\"))\n",
    "    \n",
    "    # Create a new column \"income_to_installment_ratio\" which is ratio of \n",
    "    # \"log_annual_inc\" to \"installment\"\n",
    "    df = df.withColumn(\"income_to_installment_ratio\", col(\"log_annual_inc\")/col(\"installment\"))\n",
    "    \n",
    "    # Create a new column \"int_rate_category\" which categorizes the \"int_rate\" as\n",
    "    # int_rate < 0.1:            low\n",
    "    # 0.1 <= int_rate < 0.15:    medium\n",
    "    # int_rate >= 0.15:          high    \n",
    "    df = df.withColumn(\"int_rate_category\",\n",
    "                       when(col(\"int_rate\")<0.1, \"low\")\n",
    "                       .when((col(\"int_rate\") >= 0.1) & (col(\"int_rate\") < 0.15), \"medium\" )\n",
    "                       .otherwise(\"high\")\n",
    "                       )\n",
    "    \n",
    "    # Create a new column \"high_risk_borrower\" which flags high-risk borrowers\n",
    "    # value of \"1\" based on the following conditions:\n",
    "    #   dti > 20\n",
    "    #   fico < 700\n",
    "    #   revol_util > 80\n",
    "    #   Otherwise, this column has a value of \"0\".\n",
    "    df = df.withColumn(\"high_risk_borrower\",\n",
    "                       when((col(\"dti\") > 20) | (col(\"fico\") < 700) | (col(\"revol_util\") > 80), 1) # Corrected syntax\n",
    "                       .otherwise(0)\n",
    "                      )\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def result_2(input_df):\n",
    "    '''\n",
    "    for input file: input_df is output of clean_data function\n",
    "    '''\n",
    "    print(\"--------------------------\")\n",
    "    print(\"Starting result_2\")\n",
    "    print(\"--------------------------\")\n",
    "    \n",
    "    # Calculate the \"default_rate\" for each purpose, defined as the count of loans that are\n",
    "    # not fully paid (i.e., not_fully_paid == 1) divided by the total count of loans\n",
    "    df = input_df.groupBy(\"purpose\").agg(\n",
    "         (sum(col(\"not_fully_paid\")) / count(\"*\")).alias(\"default_rate\")\n",
    "         )\n",
    "    \n",
    "    # Round the default_rate values to the two decimal values\n",
    "    df = df.withColumn(\"default_rate\", round(col(\"default_rate\"), 2))\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def redshift_load_data(data):\n",
    "    if data.count() != 0:\n",
    "        print(\"Loading the data into Redshift...\")\n",
    "        jdbcUrl = \"jdbc:redshift://emr-spark-redshift.cjgnpeot7x5i.us-east-1.redshift.amazonaws.com:5439/dev\"\n",
    "        username = \"awsuser\" #Mention redshift username\n",
    "        password = \"Awsuser1\" #Mention redshift password\n",
    "        table_name = \"result_2\" #Mention redshift table name\n",
    "    \n",
    "        # Load data from result_2 table into redshift\n",
    "        data.write \\\n",
    "            .format(\"jdbc\") \\\n",
    "            .option(\"url\", jdbcUrl) \\\n",
    "            .option(\"dbtable\", table_name) \\\n",
    "            .option(\"user\", username) \\\n",
    "            .option(\"password\", password) \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .save()\n",
    "    \n",
    "    else:\n",
    "        print(\"Empty dataframe, hence cannot load the data\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#################\n",
    "#################\n",
    "\n",
    "\n",
    "#### Import statements here\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import resample\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report \n",
    "\n",
    "import warnings\n",
    "import boto3\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "#### Import the dataset from S3\n",
    "# Build the S3 path for the dataset\n",
    "# Sample S3 URI - \"s3://bucket_name/folder_name/file_name.csv\n",
    "bucket=\"loan-data602607864436400\"\n",
    "folder_name = \"loan_cleaned_data\"\n",
    "data_key = \"loan_cleaned_data.csv\"\n",
    "data_location = f's3://{bucket}/{folder_name}/{data_key}'\n",
    "\n",
    "# Load and store the dataset into a pandas DataFrame.\n",
    "data = pd.read_csv(data_location)\n",
    "data.head()\n",
    "\n",
    "\n",
    "# Feature Engineering - One-hot Encoding\n",
    "# Convert the values in the categorical column 'purpose' into numerical format \n",
    "# using `One-hot Encoding``. The datatype of the new columns should be `int`.\n",
    "data = pd.get_dummies(data,columns=['purpose'], dtype=int)\n",
    "data.head()\n",
    "\n",
    "\n",
    "# Inspect the target column 'not_fully_paid' and identify the count of records belonging to the two classes.\n",
    "# Filter out the majority and minority classes and store them separately.\n",
    "df_majority=data[data['not_fully_paid']==0]\n",
    "df_minority=data[data['not_fully_paid']==1]\n",
    "\n",
    "\n",
    "# Handle the imbalanced data using resample method and oversample the minority class\n",
    "df_minority_upsampled = resample(df_minority,\n",
    "                                 replace=True, \n",
    "                                 n_samples = df_majority.shape[0],\n",
    "                                 random_state=42)\n",
    "\n",
    "\n",
    "# Concatenate the upsampled data records with the majority class records and shuffle the resultant dataframe\n",
    "df_balanced=pd.concat([df_majority,df_minority_upsampled])\n",
    "print(df_balanced['not_fully_paid'].value_counts())\n",
    "\n",
    "\n",
    "# Create X and y data for train-test split\n",
    "# Model Training\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import  RandomForestClassifier\n",
    "\n",
    "# Drop the columns 'sl_no' and 'not_fully_paid' and \n",
    "# create a dataframe of independent variables named X. \n",
    "# Filter the dependent variable and store it in y.\n",
    "X = df_balanced.drop(columns=['sl_no','not_fully_paid'])\n",
    "y = df_balanced['not_fully_paid']\n",
    "\n",
    "\n",
    "# Split the data \n",
    "# Split the data into training and test sets using 60:40 ratio. Use a random state equal to 42.\n",
    "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.4, random_state=42)\n",
    "\n",
    "\n",
    "# Train a Random Forest Classifier model\n",
    "# Train a Random Forest Classifier model called rf using the training data. Use a random state equal to 42.\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Predict using the trained Random Forest Classifier model\n",
    "# Predict using the trained Random Forest Classifier model rf on the test data X_test.\n",
    "from sklearn.metrics import classification_report\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "\n",
    "# Print the classification report \n",
    "# Model Evaluation\n",
    "# Evaluate the predictions by comparing it with the actual test data y_test.\n",
    "# Print the classification report to determine the evaluation metric scores.\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "\n",
    "import tempfile\n",
    "import joblib\n",
    "\n",
    "BUCKET_NAME =\"loan-data602607864436400\"\n",
    "model_name = \"model.pkl\"\n",
    "with tempfile.NamedTemporaryFile() as tmp:\n",
    "    joblib.dump(rf, tmp.name)\n",
    "    tmp.flush()\n",
    "\n",
    "    s3= boto3.client('s3')\n",
    "    \n",
    "    # Upload the model file to the specified S3 bucket named 'loan-dataYXYZYZ' (XYZXYZ can be any random integers).\n",
    "    # Ensure the model is saved as 'model.pkl' in the S3 bucket.\n",
    "    s3.upload_file(tmp.name, BUCKET_NAME, model_name)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#################\n",
    "#################\n",
    "\n",
    "def read_data(spark):\n",
    "    '''\n",
    "    spark_session : spark\n",
    "    customSchema : we have given the custom schema\n",
    "    '''\n",
    "    print(\"-------------------\")\n",
    "    print(\"Starting read data\")\n",
    "    print(\"-------------------\")\n",
    "    #Mention the Bucket name inside the bucket name variable\n",
    "    bucket_name = \"car-data843004767089968\"\n",
    "    s3_input_path = \"s3://\" + bucket_name + \"/inputfile/car_data.csv\"\n",
    "    # s3_input_path = './Car details v3.csv'\n",
    "\n",
    "    df = spark.read.csv(s3_input_path, header=True)    # Read the CSV file into a dataframe. Make sure to give only header as true\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def clean_data(input_df):\n",
    "    '''\n",
    "    for input file: input_df is output of read_data function\n",
    "    '''\n",
    "    print(\"--------------------\")\n",
    "    print(\"Starting clean_data\")\n",
    "    print(\"--------------------\")\n",
    "    df = input_df.dropna()    # Drop the rows which have null values in any of the columns.\n",
    "    df = df.dropDuplicates()    # Drop duplicate rows\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def s3_load_data(data, file_name) :\n",
    "    '''\n",
    "    data : the output data of clean_data function\n",
    "    file_name : the name of the output to be stored inside the s3\n",
    "    '''\n",
    "    #Mention the bucket name inside the bucket_name variable\n",
    "    bucket_name = \"car-data843004767089968\"\n",
    "    output_path = \"s3://\" + bucket_name + \"/output/\"+ file_name\n",
    "\n",
    "    if data.count() != 0:\n",
    "        print(\"Loading the data\", output_path)\n",
    "        #write the s3 load data command here\n",
    "        data.coalesce(1).write.csv(output_path, header=True, mode=\"overwrite\")    # Write a code to store the output to the respective locations using the output_path param.\n",
    "    else:\n",
    "        print(\"Empty dataframe, hence cannot save the data\", output_path)\n",
    "\n",
    "\n",
    "def result_1(input_df):\n",
    "    '''\n",
    "    for input file: input_df is output of clean_data function\n",
    "    '''\n",
    "    print(\"--------------------\")\n",
    "    print(\"Starting result 1\")\n",
    "    print(\"--------------------\")\n",
    "\n",
    "    # Fetch the average selling price and rename it as average_selling_price and count of cars and rename it as car_count for each car_name.\n",
    "    # And fetch only the records which have car_count greater than 2 cars.\n",
    "    df = input_df.groupBy(\"name\").agg(avg(\"selling_price\").alias(\"average_selling_price\"), count(\"*\").alias(\"car_count\")).filter(\"car_count > 2\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def result_2(input_df):\n",
    "    '''\n",
    "    for input file: input_df is output of clean_data function\n",
    "    '''\n",
    "    print(\"--------------------\")\n",
    "    print(\"Starting result_2\")\n",
    "    print(\"--------------------\")\n",
    "\n",
    "    # Create a column named as price_per_km which calculates the price per kilometer\n",
    "    df = input_df.withColumn(\"price_per_km\", col(\"selling_price\") / col(\"km_driven\"))\n",
    "\n",
    "    # Filter the data to include only rows where price_per_km is less than `10`.\n",
    "    df = df.filter(col(\"price_per_km\") < 10)\n",
    "\n",
    "    # Round the price_per_km to 2 decimals.\n",
    "    df = df.withColumn(\"price_per_km\", round(col(\"price_per_km\"), 2))\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################\n",
    "########################\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn. compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import boto3\n",
    "import pandas as pd\n",
    "from sagemaker import get_execution_role\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "\n",
    "### Create S3 path for the dataset\n",
    "# bucket = \n",
    "# data_key = \n",
    "# data_location = f's3://{bucket}/{data_key}'\n",
    "\n",
    "### Load the dataset\n",
    "data = pd.read_csv(data_location)\n",
    "\n",
    "\n",
    "### Analyze the dataset\n",
    "# Print the first few rows of the dataset to understand its structure and use inbuilt\n",
    "# functions to get insights on the dataset.\n",
    "data.head()\n",
    "\n",
    "\n",
    "### Create new feature: age of the car\n",
    "# Create a new feature 'car_age' which represents the age of the car in years.\n",
    "# The car's age can be calculated as 2024-year\n",
    "data['car_age'] = 2024 - data['year']\n",
    "\n",
    "\n",
    "### Drop the columns\n",
    "# Drop the columns 'name' and 'year' from the dataset\n",
    "del data['name'], data['year']\n",
    "\n",
    "\n",
    "### Define the features and target variable\n",
    "# Define the features `X` by dropping the target variable (selling price)\n",
    "# Define the target variable `y` as the selling price.\n",
    "X = data.drop(columns=['selling_price'])\n",
    "y = data['selling_price']\n",
    "\n",
    "\n",
    "# Identify numerical and categorical features\n",
    "numerical_features = ['km_driven', 'seats', 'car_age']\n",
    "categorical_features = ['fuel', 'seller_type', 'transmission', 'owner']\n",
    "\n",
    "### Log transformation for skewed numerical features\n",
    "X['km_driven'] = np.log1p(X['km_driven'])\n",
    "y = np.log1p(y)\n",
    "\n",
    "\n",
    "# Use StandardScaler to scale the numerical features.\n",
    "# Use OneHotEncoder to encode the categorical features, ensuring that unknown categories are handled.\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
    "    ])\n",
    "\n",
    "\n",
    "# Create a Sklearn model pipeline with the preprocessor pipeline and \n",
    "# `RandomForestRegressor` model with a random state set to `8`. \n",
    "# Store it under the name `model`.\n",
    "model = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', RandomForestRegressor(random_state=8))\n",
    "])\n",
    "\n",
    "\n",
    "### Split the data into training and test sets\n",
    "# Split the dataset into training and test sets using `train test_split` \n",
    "# with a test size of 20% and random state set to `8`\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=8)\n",
    "\n",
    "\n",
    "# Param_grid = {`regressor_n_estimators`: [100, 200, 300),\n",
    "#               `regressor_max_depth`: [None, 10, 20, 30],\n",
    "#               `regressor_min_samples_split`: [2, 5, 10)}\n",
    "# Define a parameter grid as given above with different values for \n",
    "# n_estimators, max_depth, and min_samples_split.\n",
    "param_grid = {\n",
    "    'regressor__n_estimators': [100, 200, 300],\n",
    "    'regressor__max_depth': [None, 10, 20, 30],\n",
    "    'regressor__min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "### Create the model\n",
    "# Perform hyperparameter tuning using GridSearchCV to find \n",
    "# the best parameters for the Random ForestRegressor\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='r2', n_jobs=1)\n",
    "\n",
    "\n",
    "# Fit the GridSearchCV on the training data to find the best model\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Extract the best model from the grid search results. \n",
    "# Store the best model under variable for example `best_model`.\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "\n",
    "### Make predictions\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "### Transform predictions back to original scale\n",
    "y_test = np.expm1(y_test)\n",
    "y_pred = np.expm1(y_pred)\n",
    "\n",
    "\n",
    "### Calculate evaluation metrics\n",
    "# Calculate the Mean Absolute Error (MAE), Mean Squared Error (MSE), \n",
    "# Root Mean Squared Error (RMSE), and R2 score for the model\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "### Print the metrics\n",
    "print(f'MAE: {mae}')\n",
    "print(f'MSE: {mse}')\n",
    "print(f'RMSE: {rmse} ')\n",
    "print(f'R2: {r2} ')\n",
    "print(f'Best Parameters: {grid_search.best_params_}')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
