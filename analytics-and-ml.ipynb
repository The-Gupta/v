{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12157487,"sourceType":"datasetVersion","datasetId":7656840}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-13T17:27:53.166134Z","iopub.execute_input":"2025-06-13T17:27:53.166458Z","iopub.status.idle":"2025-06-13T17:27:53.172192Z","shell.execute_reply.started":"2025-06-13T17:27:53.166439Z","shell.execute_reply":"2025-06-13T17:27:53.171593Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/vehicle/Car details v3.csv\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"\"\"\"\n# T15 Practise Solution\n\ndef read_data(spark, customSchema):\n    '''\n    spark_session: spark\n    customSchema : we have given the custom schema\n    '''\n    print(\"--------------------\")\n    print (\"Starting read_data\")\n    print(\"--------------------\")\n    \n    #Mention the Bucket name inside the bucket name variable\n    bucket_name = \"loan-data926529466287266\"\n    s3_input_ path = \"s3://\" + bucket_name + \"/inputfile/loan_data.csv\"\n    df = spark.read.csv(s3_input_path, header=True, schema=customSchema)\n    \n    return df\n\n\ndef clean_data(input_df):\n    '''\n    for input file: input_df is output of read_data function\n    '''\n    print(\"--------------------\")\n    print(\"Starting clean_data\")\n    print(\"--------------------\")\n    \n    df = input_df.dropna().dropDuplicates()\n    df = df.filter(df.purpose!='null')\n    \n    return df\n\n\ndef s3_load_data(data, file_name):\n    '''\n    data : the output data of refult_1 and result_2 function \n    file name : the name of the output to be stored inside the s3\n    '''\n    #Mention the bucket name inside the bucket_name variable\n    bucket_name = \"loan-data926529466287266\"\t\n    output_path = \"s3://\" + bucket_name + \"/output\"+ file_name\n    #output_path = \"s3://\" + bucket_name + \"/output/\"+ file_name\n    \n    if data.count() !=0:\n        print(\"Loading the data\", output_path)\n        #write the s3 load data command here\n        data.coalesce(1).write.csv(output_path, header=True, mode=\"overwrite\")\n    \n    else:\n        print(\"Empty dataframe, hence cannot save the data\", output_path)\n\n\ndef result_1(input_df):\n    '''\n    for input file: input_df is output of clean_data function\n    '''\n    print(\"--------------------------\")\n    print(\"Starting result_1\")\n    print(\"--------------------------\")\n    \n    df = input_df.filter((col(\"purpose\")==\"educational\")|(col(\"purpose\")==\"small_business\"))\n    df = df.withColumn(\"income_to_installment_ratio\", col(\"log_annual_inc\")/col(\"installment\"))\n    df = df.withColumn(\"int_rate_category\",\n                       when(col(\"int_rate\")<0.1, \"low\")\n                       .when((col(\"int_rate\") >= 0.1) & (col(\"int_rate\") < 0.15), \"medium\" )\n                       .otherwise (\"high\")\n                       )\n    \n    df = df.withColumn(\"high_risk_borrower\",\n                      when((col(\"dti\") > 20) | (col(\"fico\") < 700) | (col(\"revol_util\") > 80), 1) # Corrected syntax\n                      .otherwise(0)\n                      )\n    \n    return df\n\n\ndef result_2(input_df):\n    '''\n    for input file: input_df is output of clean_data function\n    '''\n    print(\"--------------------------\")\n    print (\"Starting result_2\")\n    print(\"--------------------------\")\n    \n    df = input_df.groupBy(\"purpose\").agg(\n         (sum(col(\"not_fully_paid\")) / count(\"*\")).alias(\"default_rate\")\n         )\n    df = df.withColumn(\"default_rate\", round(col(\"default_rate\"), 2))\n    \n    return df\n\n\ndef redshift_load_data(data):\n    if data.count() != 0:\n        print(\"Loading the data into Redshift...\")\n        jdbcUrl = \"jdbc:redshift://emr-spark-redshift.cjgnpeot7x5i.us-east-1.redshift.amazonaws.com:5439/dev\"\n        username = \"awsuser\" #Mention redshift username\n        password = \"Awsuser1\" #Mention redshift password\n        table_name = \"result_2\" #Mention redshift table name\n    \n        #Write the redshift load data command\n        data.write \\\n            .format(\"jdbc\") \\\n            .option(\"url\", jdbcUrl) \\\n            .option(\"dbtable\", table_name) \\\n            .option(\"user\", username) \\\n            .option(\"password\", password) \\\n            .mode(\"overwrite\") \\\n            .save()\n    \n    else:\n        print(\"Empty dataframe, hence cannot load the data\")\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-13T17:27:56.211282Z","iopub.status.idle":"2025-06-13T17:27:56.211700Z","shell.execute_reply.started":"2025-06-13T17:27:56.211523Z","shell.execute_reply":"2025-06-13T17:27:56.211541Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"\n#### Import statements here\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.utils import resample\nfrom sklearn.utils import shuffle\nfrom sklearn.model_selection import train_test_split \nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import confusion_matrix, classification_report \n\nimport warnings\nimport boto3\nfrom sagemaker import get_execution_role\n\nwarnings.filterwarnings('ignore')\n####\n\n\n\n#### Import the dataset from S3\nbucket=\"loan-data602607864436400\"\nfolder_name = \"loan_cleaned_data\"\ndata_key = \"loan_cleaned_data.csv\"\ndata_location = f's3://{bucket}/{folder_name}/{data_key}'\n\n\ndata = pd.read_csv(data_location)\ndata.head()\n\n\ndata = pd.get_dummies(data,columns=['purpose'], dtype=int)\ndata.head()\n\n\ndf_majority=data[data['not_fully_paid']==0]\ndf_minority=data[data['not_fully_paid']==1]\n\n\n# Handle the imbalanced data using resample method and oversample the minority class\ndf_minority_upsampled = resample(df_minority,\n                                 replace=True, \n                                 n_samples = df_majority.shape[0],\n                                 random_state=42)\n\n\n\n# Concatenate the upsampled data records with the majority class records and shuffle the resultant dataframe\ndf_balanced=pd.concat([df_majority,df_minority_upsampled])\nprint(df_balanced['not_fully_paid'].value_counts())\n\n\n# Create X and y data for train-test split\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import  RandomForestClassifier\n\nX = df_balanced.drop(columns=['sl_no','not_fully_paid'])\ny = df_balanced['not_fully_paid']\n\n\n# Split the data \nX_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.4, random_state=42)\n\n\n\n# Train a Random Forest Classifier model\nrf = RandomForestClassifier(random_state=42)\nrf.fit(X_train, y_train)\n\n\n\n# Predict using the trained Random Forest Classifier model\nfrom sklearn.metrics import classification_report\ny_pred = rf.predict(X_test)\n\n\n\n# Print the classification report \nprint(classification_report(y_test, y_pred))\n\n\n\nimport tempfile\n\nimport joblib\nBUCKET_NAME =\"loan-data602607864436400\"\nwith tempfile.NamedTemporaryFile() as tmp:\n    joblib.dump(rf, tmp.name)\n    tmp.flush()\n\n    s3= boto3.client('s3')\n    \n    s3.upload_file(tmp.name, BUCKET_NAME , \"model.pkl\")\n\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-13T17:27:56.212462Z","iopub.status.idle":"2025-06-13T17:27:56.212746Z","shell.execute_reply.started":"2025-06-13T17:27:56.212641Z","shell.execute_reply":"2025-06-13T17:27:56.212649Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn. compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\nimport boto3\nimport pandas as pd\nfrom sagemaker import get_execution_role\nimport numpy as np\nimport warnings\n\n\n### Create S3 path for the dataset\n# bucket = \n# data_key = \n# data_location = f's3://{bucket}/{data_key}'\n\n\n### Load the dataset\ndata = pd.read_csv(data_location)\n\n\n### Analyze the dataset\ndata.head()\n\n\n### Create new feature: age of the car\ndata['car_age'] = 2024 - data['year']\n\n\n### Drop the columns\ndel data['name'], data['year']\n\n\n### Define the features and target variable\nX = data.drop(columns=['selling_price'])\ny = data['selling_price']\n\n\n# numerical_features = ['km_driven', 'mileage', 'engine', 'max_power', 'seats', 'car_age']\nnumerical_features = ['km_driven', 'seats', 'car_age']\ncategorical_features = ['fuel', 'seller_type', 'transmission', 'owner']\n\n### Log transformation for skewed numerical features\nX['km_driven'] = np.log1p(X['km_driven'])\ny = np.log1p(y)\n\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', StandardScaler(), numerical_features),\n        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n    ])\n\n\nmodel = Pipeline(steps=[\n    ('preprocessor', preprocessor),\n    ('regressor', RandomForestRegressor(random_state=8))\n])\n\n### Split the data into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=8)\n\n\nparam_grid = {\n    'regressor__n_estimators': [100, 200, 300],\n    'regressor__max_depth': [None, 10, 20, 30],\n    'regressor__min_samples_split': [2, 5, 10]\n}\n\n### Create the model\ngrid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='r2', n_jobs=1)\n\n\ngrid_search.fit(X_train, y_train)\n\nbest_model = grid_search.best_estimator_\n\n\n### Make predictions\ny_pred = best_model.predict(X_test)\n\n### Transform predictions back to original scale\ny_test = np.expm1(y_test)\ny_pred = np.expm1(y_pred)\n\n\n### Calculate evaluation metrics\nmae = mean_absolute_error(y_test, y_pred)\nmse = mean_squared_error(y_test, y_pred)\nrmse = np.sqrt(mse)\nr2 = r2_score(y_test, y_pred)\n\n### Print the metrics\nprint(f'MAE: {mae}')\nprint(f'MSE: {mse}')\nprint(f'RMSE: {rmse} ')\nprint(f'R2: {r2} ')\nprint(f'Best Parameters: {grid_search.best_params_}')\n\n\"\"\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}