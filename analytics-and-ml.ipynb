{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff1524f4",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-06-09T19:12:07.754538Z",
     "iopub.status.busy": "2025-06-09T19:12:07.754167Z",
     "iopub.status.idle": "2025-06-09T19:12:10.253658Z",
     "shell.execute_reply": "2025-06-09T19:12:10.252428Z"
    },
    "papermill": {
     "duration": 2.505962,
     "end_time": "2025-06-09T19:12:10.255764",
     "exception": false,
     "start_time": "2025-06-09T19:12:07.749802",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78b06bd4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-09T19:12:10.262553Z",
     "iopub.status.busy": "2025-06-09T19:12:10.262025Z",
     "iopub.status.idle": "2025-06-09T19:12:10.272208Z",
     "shell.execute_reply": "2025-06-09T19:12:10.271117Z"
    },
    "papermill": {
     "duration": 0.015514,
     "end_time": "2025-06-09T19:12:10.274058",
     "exception": false,
     "start_time": "2025-06-09T19:12:10.258544",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# T15 Practise Solution\\n\\ndef read_data(spark, customSchema):\\n    \\'\\'\\'\\n    spark_session: spark\\n    customSchema : we have given the custom schema\\n    \\'\\'\\'\\n    print(\"--------------------\")\\n    print (\"Starting read_data\")\\n    print(\"--------------------\")\\n    \\n    #Mention the Bucket name inside the bucket name variable\\n    bucket_name = \"loan-data926529466287266\"\\n    s3_input_ path = \"s3://\" + bucket_name + \"/inputfile/loan_data.csv\"\\n    df = spark.read.csv(s3_input_path, header=True, schema=customSchema)\\n    \\n    return df\\n\\n\\ndef clean_data(input_df):\\n    \\'\\'\\'\\n    for input file: input_df is output of read_data function\\n    \\'\\'\\'\\n    print(\"--------------------\")\\n    print(\"Starting clean_data\")\\n    print(\"--------------------\")\\n    \\n    df = input_df.dropna().dropDuplicates()\\n    df = df.filter(df.purpose!=\\'null\\')\\n    \\n    return df\\n\\n\\ndef s3_load_data(data, file_name):\\n    \\'\\'\\'\\n    data : the output data of refult_1 and result_2 function \\n    file name : the name of the output to be stored inside the s3\\n    \\'\\'\\'\\n    #Mention the bucket name inside the bucket_name variable\\n    bucket_name = \"loan-data926529466287266\"\\t\\n    output_path = \"s3://\" + bucket_name + \"/output\"+ file_name\\n    #output_path = \"s3://\" + bucket_name + \"/output/\"+ file_name\\n    \\n    if data.count() !=0:\\n        print(\"Loading the data\", output_path)\\n        #write the s3 load data command here\\n        data.coalesce(1).write.csv(output_path, header=True, mode=\"overwrite\")\\n    \\n    else:\\n        print(\"Empty dataframe, hence cannot save the data\", output_path)\\n\\n\\ndef result_1(input_df):\\n    \\'\\'\\'\\n    for input file: input_df is output of clean_data function\\n    \\'\\'\\'\\n    print(\"--------------------------\")\\n    print(\"Starting result_1\")\\n    print(\"--------------------------\")\\n    \\n    df = input_df.filter((col(\"purpose\")==\"educational\")|(col(\"purpose\")==\"small_business\"))\\n    df = df.withColumn(\"income_to_installment_ratio\", col(\"log_annual_inc\")/col(\"installment\"))\\n    df = df.withColumn(\"int_rate_category\",\\n                       when(col(\"int_rate\")<0.1, \"low\")\\n                       .when((col(\"int_rate\") >= 0.1) & (col(\"int_rate\") < 0.15), \"medium\" )\\n                       .otherwise (\"high\")\\n                       )\\n    \\n    df = df.withColumn(\"high_risk_borrower\",\\n                      when((col(\"dti\") > 20) | (col(\"fico\") < 700) | (col(\"revol_util\") > 80), 1) # Corrected syntax\\n                      .otherwise(0)\\n                      )\\n    \\n    return df\\n\\n\\ndef result_2(input_df):\\n    \\'\\'\\'\\n    for input file: input_df is output of clean_data function\\n    \\'\\'\\'\\n    print(\"--------------------------\")\\n    print (\"Starting result_2\")\\n    print(\"--------------------------\")\\n    \\n    df = input_df.groupBy(\"purpose\").agg(\\n         (sum(col(\"not_fully_paid\")) / count(\"*\")).alias(\"default_rate\")\\n         )\\n    df = df.withColumn(\"default_rate\", round(col(\"default_rate\"), 2))\\n    \\n    return df\\n\\n\\ndef redshift_load_data(data):\\n    if data.count() != 0:\\n        print(\"Loading the data into Redshift...\")\\n        jdbcUrl = \"jdbc:redshift://emr-spark-redshift.cjgnpeot7x5i.us-east-1.redshift.amazonaws.com:5439/dev\"\\n        username = \"awsuser\" #Mention redshift username\\n        password = \"Awsuser1\" #Mention redshift password\\n        table_name = \"result_2\" #Mention redshift table name\\n    \\n        #Write the redshift load data command\\n        data.write             .format(\"jdbc\")             .option(\"url\", jdbcUrl)             .option(\"dbtable\", table_name)             .option(\"user\", username)             .option(\"password\", password)             .mode(\"overwrite\")             .save()\\n    \\n    else:\\n        print(\"Empty dataframe, hence cannot load the data\")\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# T15 Practise Solution\n",
    "\n",
    "def read_data(spark, customSchema):\n",
    "    '''\n",
    "    spark_session: spark\n",
    "    customSchema : we have given the custom schema\n",
    "    '''\n",
    "    print(\"--------------------\")\n",
    "    print (\"Starting read_data\")\n",
    "    print(\"--------------------\")\n",
    "    \n",
    "    #Mention the Bucket name inside the bucket name variable\n",
    "    bucket_name = \"loan-data926529466287266\"\n",
    "    s3_input_ path = \"s3://\" + bucket_name + \"/inputfile/loan_data.csv\"\n",
    "    df = spark.read.csv(s3_input_path, header=True, schema=customSchema)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def clean_data(input_df):\n",
    "    '''\n",
    "    for input file: input_df is output of read_data function\n",
    "    '''\n",
    "    print(\"--------------------\")\n",
    "    print(\"Starting clean_data\")\n",
    "    print(\"--------------------\")\n",
    "    \n",
    "    df = input_df.dropna().dropDuplicates()\n",
    "    df = df.filter(df.purpose!='null')\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def s3_load_data(data, file_name):\n",
    "    '''\n",
    "    data : the output data of refult_1 and result_2 function \n",
    "    file name : the name of the output to be stored inside the s3\n",
    "    '''\n",
    "    #Mention the bucket name inside the bucket_name variable\n",
    "    bucket_name = \"loan-data926529466287266\"\t\n",
    "    output_path = \"s3://\" + bucket_name + \"/output\"+ file_name\n",
    "    #output_path = \"s3://\" + bucket_name + \"/output/\"+ file_name\n",
    "    \n",
    "    if data.count() !=0:\n",
    "        print(\"Loading the data\", output_path)\n",
    "        #write the s3 load data command here\n",
    "        data.coalesce(1).write.csv(output_path, header=True, mode=\"overwrite\")\n",
    "    \n",
    "    else:\n",
    "        print(\"Empty dataframe, hence cannot save the data\", output_path)\n",
    "\n",
    "\n",
    "def result_1(input_df):\n",
    "    '''\n",
    "    for input file: input_df is output of clean_data function\n",
    "    '''\n",
    "    print(\"--------------------------\")\n",
    "    print(\"Starting result_1\")\n",
    "    print(\"--------------------------\")\n",
    "    \n",
    "    df = input_df.filter((col(\"purpose\")==\"educational\")|(col(\"purpose\")==\"small_business\"))\n",
    "    df = df.withColumn(\"income_to_installment_ratio\", col(\"log_annual_inc\")/col(\"installment\"))\n",
    "    df = df.withColumn(\"int_rate_category\",\n",
    "                       when(col(\"int_rate\")<0.1, \"low\")\n",
    "                       .when((col(\"int_rate\") >= 0.1) & (col(\"int_rate\") < 0.15), \"medium\" )\n",
    "                       .otherwise (\"high\")\n",
    "                       )\n",
    "    \n",
    "    df = df.withColumn(\"high_risk_borrower\",\n",
    "                      when((col(\"dti\") > 20) | (col(\"fico\") < 700) | (col(\"revol_util\") > 80), 1) # Corrected syntax\n",
    "                      .otherwise(0)\n",
    "                      )\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def result_2(input_df):\n",
    "    '''\n",
    "    for input file: input_df is output of clean_data function\n",
    "    '''\n",
    "    print(\"--------------------------\")\n",
    "    print (\"Starting result_2\")\n",
    "    print(\"--------------------------\")\n",
    "    \n",
    "    df = input_df.groupBy(\"purpose\").agg(\n",
    "         (sum(col(\"not_fully_paid\")) / count(\"*\")).alias(\"default_rate\")\n",
    "         )\n",
    "    df = df.withColumn(\"default_rate\", round(col(\"default_rate\"), 2))\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def redshift_load_data(data):\n",
    "    if data.count() != 0:\n",
    "        print(\"Loading the data into Redshift...\")\n",
    "        jdbcUrl = \"jdbc:redshift://emr-spark-redshift.cjgnpeot7x5i.us-east-1.redshift.amazonaws.com:5439/dev\"\n",
    "        username = \"awsuser\" #Mention redshift username\n",
    "        password = \"Awsuser1\" #Mention redshift password\n",
    "        table_name = \"result_2\" #Mention redshift table name\n",
    "    \n",
    "        #Write the redshift load data command\n",
    "        data.write \\\n",
    "            .format(\"jdbc\") \\\n",
    "            .option(\"url\", jdbcUrl) \\\n",
    "            .option(\"dbtable\", table_name) \\\n",
    "            .option(\"user\", username) \\\n",
    "            .option(\"password\", password) \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .save()\n",
    "    \n",
    "    else:\n",
    "        print(\"Empty dataframe, hence cannot load the data\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db52cff5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-09T19:12:10.280437Z",
     "iopub.status.busy": "2025-06-09T19:12:10.280073Z",
     "iopub.status.idle": "2025-06-09T19:12:10.287728Z",
     "shell.execute_reply": "2025-06-09T19:12:10.286749Z"
    },
    "papermill": {
     "duration": 0.01294,
     "end_time": "2025-06-09T19:12:10.289519",
     "exception": false,
     "start_time": "2025-06-09T19:12:10.276579",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#### Import statements here\\nimport pandas as pd\\nimport matplotlib.pyplot as plt\\nfrom sklearn.utils import resample\\nfrom sklearn.utils import shuffle\\nfrom sklearn.model_selection import train_test_split \\nfrom sklearn.ensemble import RandomForestClassifier\\nfrom sklearn.metrics import confusion_matrix, classification_report \\n\\nimport warnings\\nimport boto3\\nfrom sagemaker import get_execution_role\\n\\nwarnings.filterwarnings(\\'ignore\\')\\n####\\n\\n\\n\\n#### Import the dataset from S3\\nbucket=\"loan-data602607864436400\"\\nfolder_name = \"loan_cleaned_data\"\\ndata_key = \"loan_cleaned_data.csv\"\\ndata_location = f\\'s3://{bucket}/{folder_name}/{data_key}\\'\\n\\n\\ndata = pd.read_csv(data_location)\\ndata.head()\\n\\n\\ndata = pd.get_dummies(data,columns=[\\'purpose\\'], dtype=int)\\ndata.head()\\n\\n\\ndf_majority=data[data[\\'not_fully_paid\\']==0]\\ndf_minority=data[data[\\'not_fully_paid\\']==1]\\n\\n\\n# Handle the imbalanced data using resample method and oversample the minority class\\ndf_minority_upsampled = resample(df_minority,\\n                                 replace=True, \\n                                 n_samples = df_majority.shape[0],\\n                                 random_state=42)\\n\\n\\n\\n# Concatenate the upsampled data records with the majority class records and shuffle the resultant dataframe\\ndf_balanced=pd.concat([df_majority,df_minority_upsampled])\\nprint(df_balanced[\\'not_fully_paid\\'].value_counts())\\n\\n\\n# Create X and y data for train-test split\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.ensemble import  RandomForestClassifier\\n\\nX = df_balanced.drop(columns=[\\'sl_no\\',\\'not_fully_paid\\'])\\ny = df_balanced[\\'not_fully_paid\\']\\n\\n\\n# Split the data \\nX_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.4, random_state=42)\\n\\n\\n\\n# Train a Random Forest Classifier model\\nrf = RandomForestClassifier(random_state=42)\\nrf.fit(X_train, y_train)\\n\\n\\n\\n# Predict using the trained Random Forest Classifier model\\nfrom sklearn.metrics import classification_report\\ny_pred = rf.predict(X_test)\\n\\n\\n\\n# Print the classification report \\nprint(classification_report(y_test, y_pred))\\n\\n\\n\\nimport tempfile\\n\\nimport joblib\\nBUCKET_NAME =\"loan-data602607864436400\"\\nwith tempfile.NamedTemporaryFile() as tmp:\\n    joblib.dump(rf, tmp.name)\\n    tmp.flush()\\n\\n    s3= boto3.client(\\'s3\\')\\n    \\n    s3.upload_file(tmp.name, BUCKET_NAME , \"model.pkl\")\\n\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "#### Import statements here\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import resample\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report \n",
    "\n",
    "import warnings\n",
    "import boto3\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "####\n",
    "\n",
    "\n",
    "\n",
    "#### Import the dataset from S3\n",
    "bucket=\"loan-data602607864436400\"\n",
    "folder_name = \"loan_cleaned_data\"\n",
    "data_key = \"loan_cleaned_data.csv\"\n",
    "data_location = f's3://{bucket}/{folder_name}/{data_key}'\n",
    "\n",
    "\n",
    "data = pd.read_csv(data_location)\n",
    "data.head()\n",
    "\n",
    "\n",
    "data = pd.get_dummies(data,columns=['purpose'], dtype=int)\n",
    "data.head()\n",
    "\n",
    "\n",
    "df_majority=data[data['not_fully_paid']==0]\n",
    "df_minority=data[data['not_fully_paid']==1]\n",
    "\n",
    "\n",
    "# Handle the imbalanced data using resample method and oversample the minority class\n",
    "df_minority_upsampled = resample(df_minority,\n",
    "                                 replace=True, \n",
    "                                 n_samples = df_majority.shape[0],\n",
    "                                 random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "# Concatenate the upsampled data records with the majority class records and shuffle the resultant dataframe\n",
    "df_balanced=pd.concat([df_majority,df_minority_upsampled])\n",
    "print(df_balanced['not_fully_paid'].value_counts())\n",
    "\n",
    "\n",
    "# Create X and y data for train-test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import  RandomForestClassifier\n",
    "\n",
    "X = df_balanced.drop(columns=['sl_no','not_fully_paid'])\n",
    "y = df_balanced['not_fully_paid']\n",
    "\n",
    "\n",
    "# Split the data \n",
    "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.4, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "# Train a Random Forest Classifier model\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "\n",
    "# Predict using the trained Random Forest Classifier model\n",
    "from sklearn.metrics import classification_report\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "# Print the classification report \n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "\n",
    "\n",
    "import tempfile\n",
    "\n",
    "import joblib\n",
    "BUCKET_NAME =\"loan-data602607864436400\"\n",
    "with tempfile.NamedTemporaryFile() as tmp:\n",
    "    joblib.dump(rf, tmp.name)\n",
    "    tmp.flush()\n",
    "\n",
    "    s3= boto3.client('s3')\n",
    "    \n",
    "    s3.upload_file(tmp.name, BUCKET_NAME , \"model.pkl\")\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b33a1b3",
   "metadata": {
    "papermill": {
     "duration": 0.002126,
     "end_time": "2025-06-09T19:12:10.294189",
     "exception": false,
     "start_time": "2025-06-09T19:12:10.292063",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ebba08",
   "metadata": {
    "papermill": {
     "duration": 0.001931,
     "end_time": "2025-06-09T19:12:10.298532",
     "exception": false,
     "start_time": "2025-06-09T19:12:10.296601",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 11.108017,
   "end_time": "2025-06-09T19:12:11.023792",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-06-09T19:11:59.915775",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
